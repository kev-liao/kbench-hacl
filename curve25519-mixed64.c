/* This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
 * KreMLin invocation: /home/bhargava/Desktop/repositories/kremlin/krml -funroll-loops 8 -warn-error +9 -I /home/bhargava/Desktop/repositories/hacl-star/lib/ -I /home/bhargava/Desktop/repositories/hacl-star/lib/fst -I /home/bhargava/Desktop/repositories/kremlin/kremlib -I /home/bhargava/Desktop/repositories/hacl-star/specs -I ./lemmas -I . -ccopt -march=native -fbuiltin-uint128 -drop FStar.UInt128 -fnocompound-literals -fc89-scope -fparentheses -fcurly-braces -tmpdir curve64-c curve64-c/out.krml -skip-compilation -minimal -add-include "kremlib.h" -bundle Hacl.Curve25519_64=* -drop Hacl.Impl.Curve25519.Field64.Core -no-prefix Hacl.Impl.Curve25519.Field64.Core -add-include "vale_25519.h"
 * F* version: 1014d13d
 * KreMLin version: d0fcbc62
 */

#include <linux/kernel.h>
#include <linux/string.h>
enum { CURVE25519_POINT_SIZE = 32 };
typedef __uint128_t u128;
typedef uint8_t u8;
typedef uint32_t u32;
typedef uint64_t u64;

#define load64_le(b_i) le64_to_cpup((__force __le64 *)b_i)
#define store64_le(b_o,o) *(__force __le64 *)b_o = cpu_to_le64(o)
#define KRML_CHECK_SIZE(a,b) 0;

#define inline __always_inline

inline static void fadd(uint64_t *const c, const uint64_t *const a, const uint64_t *const b) {
  asm volatile(
    "mov     $38, %%eax ;"
    "xorl  %%ecx, %%ecx ;"
    "movq   (%2),  %%r8 ;"  "adcx   (%1),  %%r8 ;"
    "movq  8(%2),  %%r9 ;"  "adcx  8(%1),  %%r9 ;"
    "movq 16(%2), %%r10 ;"  "adcx 16(%1), %%r10 ;"
    "movq 24(%2), %%r11 ;"  "adcx 24(%1), %%r11 ;"
    "cmovc %%eax, %%ecx ;"
    "xorl %%eax, %%eax  ;"
    "adcx %%rcx,  %%r8  ;"
    "adcx %%rax,  %%r9  ;"  "movq  %%r9,  8(%0) ;"
    "adcx %%rax, %%r10  ;"  "movq %%r10, 16(%0) ;"
    "adcx %%rax, %%r11  ;"  "movq %%r11, 24(%0) ;"
    "mov     $38, %%ecx ;"
    "cmovc %%ecx, %%eax ;"
    "addq %%rax,  %%r8  ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rcx", "%r8", "%r9", "%r10", "%r11"
  );
}

inline static void fsub(uint64_t *const c, const uint64_t *const a, const uint64_t *const b) {
  asm volatile(
    "mov     $38, %%eax ;"
    "movq   (%1),  %%r8 ;"  "subq   (%2),  %%r8 ;"
    "movq  8(%1),  %%r9 ;"  "sbbq  8(%2),  %%r9 ;"
    "movq 16(%1), %%r10 ;"  "sbbq 16(%2), %%r10 ;"
    "movq 24(%1), %%r11 ;"  "sbbq 24(%2), %%r11 ;"
    "mov      $0, %%ecx ;"
    "cmovc %%eax, %%ecx ;"
    "subq %%rcx,  %%r8  ;"
    "sbbq    $0,  %%r9  ;"  "movq  %%r9,  8(%0) ;"
    "sbbq    $0, %%r10  ;"  "movq %%r10, 16(%0) ;"
    "sbbq    $0, %%r11  ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx  ;"
    "cmovc %%eax, %%ecx ;"
    "subq %%rcx,  %%r8  ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rcx", "%r8", "%r9", "%r10", "%r11"
  );
}

static void mul(uint64_t *const c, const uint64_t *const a, const uint64_t *const b) {
  asm volatile(
    "movq   (%1), %%rdx; " /* A[0] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[0]*B[0] */    "xorl %%r10d, %%r10d ;"                           "movq  %%r8,  (%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[0]*B[1] */    "adox  %%r9, %%r10 ;"                             "movq %%r10, 8(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[0]*B[2] */    "adox %%r11, %%r12 ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[0]*B[3] */    "adox %%r13, %%r14 ;"                                                       "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"

    "movq  8(%1), %%rdx; " /* A[1] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[1]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 8(%0),  %%r8 ;"    "movq  %%r8,  8(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[1]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 16(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[1]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"                              "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[1]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"                              "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"

    "movq 16(%1), %%rdx; " /* A[2] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[2]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 16(%0), %%r8 ;"    "movq  %%r8, 16(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[2]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 24(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[2]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"                              "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[2]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"                              "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"

    "movq 24(%1), %%rdx; " /* A[3] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[3]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 24(%0), %%r8 ;"    "movq  %%r8, 24(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[3]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 32(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[3]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"    "movq %%r12, 40(%0) ;"    "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[3]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"    "movq %%r14, 48(%0) ;"    "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"    "movq %%rax, 56(%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rdx", "%r8",
    "%r9", "%r10", "%r11", "%r12", "%r13", "%r14"
  );
}

static void mul2(uint64_t *const c, const uint64_t *const a, const uint64_t *const b) {
  asm volatile(
    "xorl %%r14d, %%r14d ;"
    "movq   (%1), %%rdx; " /* A[0] */
    "mulx   (%2),  %%r8, %%r12; " /* A[0]*B[0] */  "xorl %%r10d, %%r10d ;"  "movq %%r8, (%0) ;"
    "mulx  8(%2), %%r10, %%rax; " /* A[0]*B[1] */  "adox %%r10, %%r12 ;"
    "mulx 16(%2),  %%r8, %%rbx; " /* A[0]*B[2] */  "adox  %%r8, %%rax ;"
    "mulx 24(%2), %%r10, %%rcx; " /* A[0]*B[3] */  "adox %%r10, %%rbx ;"
    /*******************************************/  "adox %%r14, %%rcx ;"

    "movq  8(%1), %%rdx; " /* A[1] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[1]*B[0] */  "adox %%r12,  %%r8 ;"  "movq  %%r8, 8(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[1]*B[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rax ;"
    "mulx 16(%2),  %%r8, %%r13; " /* A[1]*B[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%rbx ;"
    "mulx 24(%2), %%r10, %%r12; " /* A[1]*B[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%rcx ;"
    /*******************************************/  "adox %%r14, %%r12 ;"  "adcx %%r14, %%r12 ;"

    "movq 16(%1), %%rdx; " /* A[2] */              "xorl %%r10d, %%r10d ;"
    "mulx   (%2),  %%r8,  %%r9; " /* A[2]*B[0] */  "adox %%rax,  %%r8 ;"  "movq %%r8, 16(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[2]*B[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rbx ;"
    "mulx 16(%2),  %%r8, %%r13; " /* A[2]*B[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%rcx ;"
    "mulx 24(%2), %%r10, %%rax; " /* A[2]*B[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%r12 ;"
    /*******************************************/  "adox %%r14, %%rax ;"  "adcx %%r14, %%rax ;"

    "movq 24(%1), %%rdx; " /* A[3] */              "xorl %%r10d, %%r10d ;"
    "mulx   (%2),  %%r8,  %%r9; " /* A[3]*B[0] */  "adox %%rbx,  %%r8 ;"  "movq %%r8, 24(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[3]*B[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rcx ;"  "movq %%rcx, 32(%0) ;"
    "mulx 16(%2),  %%r8, %%r13; " /* A[3]*B[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%r12 ;"  "movq %%r12, 40(%0) ;"
    "mulx 24(%2), %%r10, %%rbx; " /* A[3]*B[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%rax ;"  "movq %%rax, 48(%0) ;"
    /*******************************************/  "adox %%r14, %%rbx ;"  "adcx %%r14, %%rbx ;"  "movq %%rbx, 56(%0) ;"

    "movq 32(%1), %%rdx; " /* C[0] */
    "mulx 32(%2),  %%r8, %%r12; " /* C[0]*D[0] */  "xorl %%r10d, %%r10d ;" "movq %%r8, 64(%0);"
    "mulx 40(%2), %%r10, %%rax; " /* C[0]*D[1] */  "adox %%r10, %%r12 ;"
    "mulx 48(%2),  %%r8, %%rbx; " /* C[0]*D[2] */  "adox  %%r8, %%rax ;"
    "mulx 56(%2), %%r10, %%rcx; " /* C[0]*D[3] */  "adox %%r10, %%rbx ;"
    /*******************************************/  "adox %%r14, %%rcx ;"

    "movq 40(%1), %%rdx; " /* C[1] */              "xorl %%r10d, %%r10d ;"
    "mulx 32(%2),  %%r8,  %%r9; " /* C[1]*D[0] */  "adox %%r12,  %%r8 ;"  "movq  %%r8, 72(%0);"
    "mulx 40(%2), %%r10, %%r11; " /* C[1]*D[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rax ;"
    "mulx 48(%2),  %%r8, %%r13; " /* C[1]*D[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%rbx ;"
    "mulx 56(%2), %%r10, %%r12; " /* C[1]*D[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%rcx ;"
    /*******************************************/  "adox %%r14, %%r12 ;"  "adcx %%r14, %%r12 ;"

    "movq 48(%1), %%rdx; " /* C[2] */              "xorl %%r10d, %%r10d ;"
    "mulx 32(%2),  %%r8,  %%r9; " /* C[2]*D[0] */  "adox %%rax,  %%r8 ;"  "movq  %%r8, 80(%0);"
    "mulx 40(%2), %%r10, %%r11; " /* C[2]*D[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rbx ;"
    "mulx 48(%2),  %%r8, %%r13; " /* C[2]*D[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%rcx ;"
    "mulx 56(%2), %%r10, %%rax; " /* C[2]*D[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%r12 ;"
    /*******************************************/  "adox %%r14, %%rax ;"  "adcx %%r14, %%rax ;"

    "movq 56(%1), %%rdx; " /* C[3] */              "xorl %%r10d, %%r10d ;"
    "mulx 32(%2),  %%r8,  %%r9; " /* C[3]*D[0] */  "adox %%rbx,  %%r8 ;"  "movq  %%r8, 88(%0);"
    "mulx 40(%2), %%r10, %%r11; " /* C[3]*D[1] */  "adox %%r10,  %%r9 ;"  "adcx  %%r9, %%rcx ;"  "movq %%rcx,  96(%0) ;"
    "mulx 48(%2),  %%r8, %%r13; " /* C[3]*D[2] */  "adox  %%r8, %%r11 ;"  "adcx %%r11, %%r12 ;"  "movq %%r12, 104(%0) ;"
    "mulx 56(%2), %%r10, %%rbx; " /* C[3]*D[3] */  "adox %%r10, %%r13 ;"  "adcx %%r13, %%rax ;"  "movq %%rax, 112(%0) ;"
    /*******************************************/  "adox %%r14, %%rbx ;"  "adcx %%r14, %%rbx ;"  "movq %%rbx, 120(%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rbx", "%rcx", "%rdx",
    "%r8", "%r9", "%r10", "%r11", "%r12", "%r13", "%r14"
  );
}

static 
void sqr(uint64_t *const c, const uint64_t *const a) {
  asm volatile(
    "movq   (%1), %%rdx        ;" /* A[0]      */
    "mulx  8(%1),  %%r8, %%r14 ;" /* A[1]*A[0] */  "xorl %%r15d, %%r15d;"
    "mulx 16(%1),  %%r9, %%r10 ;" /* A[2]*A[0] */  "adcx %%r14,  %%r9 ;"
    "mulx 24(%1), %%rax, %%rcx ;" /* A[3]*A[0] */  "adcx %%rax, %%r10 ;"
    "movq 24(%1), %%rdx        ;" /* A[3]      */
    "mulx  8(%1), %%r11, %%r12 ;" /* A[1]*A[3] */  "adcx %%rcx, %%r11 ;"
    "mulx 16(%1), %%rax, %%r13 ;" /* A[2]*A[3] */  "adcx %%rax, %%r12 ;"
    "movq  8(%1), %%rdx        ;" /* A[1]      */  "adcx %%r15, %%r13 ;"
    "mulx 16(%1), %%rax, %%rcx ;" /* A[2]*A[1] */  "movq    $0, %%r14 ;"
    /*******************************************/  "adcx %%r15, %%r14 ;"

    "xorl %%r15d, %%r15d;"
    "adox %%rax, %%r10 ;"  "adcx  %%r8,  %%r8 ;"
    "adox %%rcx, %%r11 ;"  "adcx  %%r9,  %%r9 ;"
    "adox %%r15, %%r12 ;"  "adcx %%r10, %%r10 ;"
    "adox %%r15, %%r13 ;"  "adcx %%r11, %%r11 ;"
    "adox %%r15, %%r14 ;"  "adcx %%r12, %%r12 ;"
                           "adcx %%r13, %%r13 ;"
                           "adcx %%r14, %%r14 ;"

    "movq   (%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[0]^2 */
    /********************/  "movq %%rax,  0(%0) ;"
    "addq %%rcx,  %%r8 ;"   "movq  %%r8,  8(%0) ;"
    "movq  8(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[1]^2 */
    "adcq %%rax,  %%r9 ;"   "movq  %%r9, 16(%0) ;"
    "adcq %%rcx, %%r10 ;"   "movq %%r10, 24(%0) ;"
    "movq 16(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[2]^2 */
    "adcq %%rax, %%r11 ;"   "movq %%r11, 32(%0) ;"
    "adcq %%rcx, %%r12 ;"   "movq %%r12, 40(%0) ;"
    "movq 24(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[3]^2 */
    "adcq %%rax, %%r13 ;"   "movq %%r13, 48(%0) ;"
    "adcq %%rcx, %%r14 ;"   "movq %%r14, 56(%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rcx", "%rdx",
    "%r8", "%r9", "%r10", "%r11", "%r12", "%r13", "%r14", "%r15"
  );
}

static 
void sqr2(uint64_t *const c, const uint64_t *const a) {
  asm volatile(
    "movq   (%1), %%rdx        ;" /* A[0]      */
    "mulx  8(%1),  %%r8, %%r14 ;" /* A[1]*A[0] */  "xorl %%r15d, %%r15d;"
    "mulx 16(%1),  %%r9, %%r10 ;" /* A[2]*A[0] */  "adcx %%r14,  %%r9 ;"
    "mulx 24(%1), %%rax, %%rcx ;" /* A[3]*A[0] */  "adcx %%rax, %%r10 ;"
    "movq 24(%1), %%rdx        ;" /* A[3]      */
    "mulx  8(%1), %%r11, %%r12 ;" /* A[1]*A[3] */  "adcx %%rcx, %%r11 ;"
    "mulx 16(%1), %%rax, %%r13 ;" /* A[2]*A[3] */  "adcx %%rax, %%r12 ;"
    "movq  8(%1), %%rdx        ;" /* A[1]      */  "adcx %%r15, %%r13 ;"
    "mulx 16(%1), %%rax, %%rcx ;" /* A[2]*A[1] */  "movq    $0, %%r14 ;"
    /*******************************************/  "adcx %%r15, %%r14 ;"

    "xorl %%r15d, %%r15d;"
    "adox %%rax, %%r10 ;"  "adcx  %%r8,  %%r8 ;"
    "adox %%rcx, %%r11 ;"  "adcx  %%r9,  %%r9 ;"
    "adox %%r15, %%r12 ;"  "adcx %%r10, %%r10 ;"
    "adox %%r15, %%r13 ;"  "adcx %%r11, %%r11 ;"
    "adox %%r15, %%r14 ;"  "adcx %%r12, %%r12 ;"
                           "adcx %%r13, %%r13 ;"
                           "adcx %%r14, %%r14 ;"

    "movq   (%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[0]^2 */
    /********************/  "movq %%rax,  0(%0) ;"
    "addq %%rcx,  %%r8 ;"   "movq  %%r8,  8(%0) ;"
    "movq  8(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[1]^2 */
    "adcq %%rax,  %%r9 ;"   "movq  %%r9, 16(%0) ;"
    "adcq %%rcx, %%r10 ;"   "movq %%r10, 24(%0) ;"
    "movq 16(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[2]^2 */
    "adcq %%rax, %%r11 ;"   "movq %%r11, 32(%0) ;"
    "adcq %%rcx, %%r12 ;"   "movq %%r12, 40(%0) ;"
    "movq 24(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[3]^2 */
    "adcq %%rax, %%r13 ;"   "movq %%r13, 48(%0) ;"
    "adcq %%rcx, %%r14 ;"   "movq %%r14, 56(%0) ;"


    "movq 32(%1), %%rdx        ;" /* B[0]      */
    "mulx 40(%1),  %%r8, %%r14 ;" /* B[1]*B[0] */  "xorl %%r15d, %%r15d;"
    "mulx 48(%1),  %%r9, %%r10 ;" /* B[2]*B[0] */  "adcx %%r14,  %%r9 ;"
    "mulx 56(%1), %%rax, %%rcx ;" /* B[3]*B[0] */  "adcx %%rax, %%r10 ;"
    "movq 56(%1), %%rdx        ;" /* B[3]      */
    "mulx 40(%1), %%r11, %%r12 ;" /* B[1]*B[3] */  "adcx %%rcx, %%r11 ;"
    "mulx 48(%1), %%rax, %%r13 ;" /* B[2]*B[3] */  "adcx %%rax, %%r12 ;"
    "movq 40(%1), %%rdx        ;" /* B[1]      */  "adcx %%r15, %%r13 ;"
    "mulx 48(%1), %%rax, %%rcx ;" /* B[2]*B[1] */  "movq    $0, %%r14 ;"
    /*******************************************/  "adcx %%r15, %%r14 ;"

    "xorl %%r15d, %%r15d;"
    "adox %%rax, %%r10 ;"  "adcx  %%r8,  %%r8 ;"
    "adox %%rcx, %%r11 ;"  "adcx  %%r9,  %%r9 ;"
    "adox %%r15, %%r12 ;"  "adcx %%r10, %%r10 ;"
    "adox %%r15, %%r13 ;"  "adcx %%r11, %%r11 ;"
    "adox %%r15, %%r14 ;"  "adcx %%r12, %%r12 ;"
                           "adcx %%r13, %%r13 ;"
                           "adcx %%r14, %%r14 ;"

    "movq 32(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* B[0]^2 */
    /********************/  "movq %%rax,  64(%0) ;"
    "addq %%rcx,  %%r8 ;"   "movq  %%r8,  72(%0) ;"
    "movq 40(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* B[1]^2 */
    "adcq %%rax,  %%r9 ;"   "movq  %%r9,  80(%0) ;"
    "adcq %%rcx, %%r10 ;"   "movq %%r10,  88(%0) ;"
    "movq 48(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* B[2]^2 */
    "adcq %%rax, %%r11 ;"   "movq %%r11,  96(%0) ;"
    "adcq %%rcx, %%r12 ;"   "movq %%r12, 104(%0) ;"
    "movq 56(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* B[3]^2 */
    "adcq %%rax, %%r13 ;"   "movq %%r13, 112(%0) ;"
    "adcq %%rcx, %%r14 ;"   "movq %%r14, 120(%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rcx", "%rdx",
    "%r8", "%r9", "%r10", "%r11", "%r12", "%r13", "%r14", "%r15"
  );
}

static 
void carry_wide(uint64_t *const c, const uint64_t *const a) {
  asm volatile(
    "movl    $38, %%edx ;" /* 2*c = 38 = 2^256 */
    "mulx 32(%1),  %%r8, %%r10 ;" /* c*C[4] */  "xorl %%ebx, %%ebx ;"  "adox   (%1),  %%r8 ;"
    "mulx 40(%1),  %%r9, %%r11 ;" /* c*C[5] */  "adcx %%r10,  %%r9 ;"  "adox  8(%1),  %%r9 ;"
    "mulx 48(%1), %%r10, %%rax ;" /* c*C[6] */  "adcx %%r11, %%r10 ;"  "adox 16(%1), %%r10 ;"
    "mulx 56(%1), %%r11, %%rcx ;" /* c*C[7] */  "adcx %%rax, %%r11 ;"  "adox 24(%1), %%r11 ;"
    /****************************************/  "adcx %%rbx, %%rcx ;"  "adox  %%rbx, %%rcx ;"
    "imul %%rdx, %%rcx ;" /* c*C[4], cf=0, of=0 */
    "adcx %%rcx,  %%r8 ;"
    "adcx %%rbx,  %%r9 ;"  "movq  %%r9,  8(%0) ;"
    "adcx %%rbx, %%r10 ;"  "movq %%r10, 16(%0) ;"
    "adcx %%rbx, %%r11 ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rbx", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11"
  );
}

static 
void carry_wide2(uint64_t *const c, const uint64_t *const a) {
  asm volatile(
    "movl    $38, %%edx; " /* 2*c = 38 = 2^256 */
    "mulx 32(%1),  %%r8, %%r10; " /* c*C[4] */   "xorl %%ebx, %%ebx ;"  "adox   (%1),  %%r8 ;"
    "mulx 40(%1),  %%r9, %%r11; " /* c*C[5] */   "adcx %%r10,  %%r9 ;"  "adox  8(%1),  %%r9 ;"
    "mulx 48(%1), %%r10, %%rax; " /* c*C[6] */   "adcx %%r11, %%r10 ;"  "adox 16(%1), %%r10 ;"
    "mulx 56(%1), %%r11, %%rcx; " /* c*C[7] */   "adcx %%rax, %%r11 ;"  "adox 24(%1), %%r11 ;"
    /****************************************/   "adcx %%rbx, %%rcx ;"  "adox  %%rbx, %%rcx ;"
    "imul %%rdx, %%rcx ;" /* c*C[4], cf=0, of=0 */
    "adcx %%rcx,  %%r8 ;"
    "adcx %%rbx,  %%r9 ;"  "movq  %%r9,  8(%0) ;"
    "adcx %%rbx, %%r10 ;"  "movq %%r10, 16(%0) ;"
    "adcx %%rbx, %%r11 ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8,   (%0) ;"

    "mulx  96(%1),  %%r8, %%r10; " /* c*C[4] */  "xorl %%ebx, %%ebx ;"  "adox 64(%1),  %%r8 ;"
    "mulx 104(%1),  %%r9, %%r11; " /* c*C[5] */  "adcx %%r10,  %%r9 ;"  "adox 72(%1),  %%r9 ;"
    "mulx 112(%1), %%r10, %%rax; " /* c*C[6] */  "adcx %%r11, %%r10 ;"  "adox 80(%1), %%r10 ;"
    "mulx 120(%1), %%r11, %%rcx; " /* c*C[7] */  "adcx %%rax, %%r11 ;"  "adox 88(%1), %%r11 ;"
    /*****************************************/  "adcx %%rbx, %%rcx ;"  "adox  %%rbx, %%rcx ;"
    "imul %%rdx, %%rcx ;" /* c*C[4], cf=0, of=0 */
    "adcx %%rcx,  %%r8 ;"
    "adcx %%rbx,  %%r9 ;"  "movq  %%r9, 40(%0) ;"
    "adcx %%rbx, %%r10 ;"  "movq %%r10, 48(%0) ;"
    "adcx %%rbx, %%r11 ;"  "movq %%r11, 56(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8, 32(%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rbx", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11"
  );
}

inline static void fmul1(uint64_t *const c, const uint64_t *const a, uint64_t ignored) {
  const uint64_t a24 = 121665;
  asm volatile(
    "movq     %2, %%rdx ;"
    "mulx   (%1),  %%r8, %%r10 ;"
    "mulx  8(%1),  %%r9, %%r11 ;"  "addq %%r10,  %%r9 ;"
    "mulx 16(%1), %%r10, %%rax ;"  "adcq %%r11, %%r10 ;"
    "mulx 24(%1), %%r11, %%rcx ;"  "adcq %%rax, %%r11 ;"
    /***************************/  "adcq    $0, %%rcx ;"
    "movl   $38, %%edx ;" /* 2*c = 38 = 2^256 mod 2^255-19*/
    "imul %%rdx, %%rcx ;"
    "addq %%rcx,  %%r8 ;"
    "adcq    $0,  %%r9 ;"  "movq  %%r9,  8(%0) ;"
    "adcq    $0, %%r10 ;"  "movq %%r10, 16(%0) ;"
    "adcq    $0, %%r11 ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (a24)
  : "memory", "cc", "%rax", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11"
  );
}

inline static uint64_t add1(uint64_t *const c, const uint64_t* a, uint64_t x) {
  uint64_t carry;
  asm volatile (
    /* Add either 19 or 38 to c */
    "addq    %4,   %0 ;"
    "adcq    $0,   %1 ;"
    "adcq    $0,   %2 ;"
    "adcq    $0,   %3 ;"
    "adcq    $0,   %5 ;"

    : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r" (x), "=r"(carry)
    :
    : "memory", "cc"
  );
  return carry;
}

inline static 
void fmul(uint64_t* dst, uint64_t* in_a, uint64_t* in_b, uint64_t* tmp) {
  //  __aligned(32) uint64_t tmp[8] = {0};
  mul(tmp,in_a,in_b);
  carry_wide(dst,tmp);
}


inline static 
void fmul2(uint64_t* dst, uint64_t* in_a, uint64_t* in_b, uint64_t* tmp) {
  //  __aligned(32) uint64_t tmp[16] = {0};
  mul2(tmp,in_a,in_b);
  carry_wide(dst,tmp);
  carry_wide(dst+4,tmp+8);
//  carry_wide2(dst,tmp);
}

inline static 
void fsqr(uint64_t* dst, uint64_t* in_a, uint64_t* tmp) {
  //  __aligned(32) uint64_t tmp[8] = {0};
  sqr(tmp,in_a);
  carry_wide(dst,tmp);
}

inline static 
void fsqr2(uint64_t* dst, uint64_t* in_a, uint64_t* tmp) {
  //  __aligned(32) uint64_t tmp[16] = {0};
  sqr2(tmp,in_a);
  carry_wide(dst,tmp);
  carry_wide(dst+4,tmp+8);
//  carry_wide2(dst,tmp);
}


static inline void cswap1(uint8_t bit, uint64_t *const p0, uint64_t *const p1) {
  uint64_t temp;
  __asm__ __volatile__(
    "test %9, %9 ;"
    "movq %0, %8 ;"
    "cmovnzq %4, %0 ;"
    "cmovnzq %8, %4 ;"
    "movq %1, %8 ;"
    "cmovnzq %5, %1 ;"
    "cmovnzq %8, %5 ;"
    "movq %2, %8 ;"
    "cmovnzq %6, %2 ;"
    "cmovnzq %8, %6 ;"
    "movq %3, %8 ;"
    "cmovnzq %7, %3 ;"
    "cmovnzq %8, %7 ;"
    : "+r"(p0[0]), "+r"(p0[1]), "+r"(p0[2]), "+r"(p0[3]),
      "+r"(p1[0]), "+r"(p1[1]), "+r"(p1[2]), "+r"(p1[3]),
      "=r"(temp)
    : "r"(bit)
    : "cc"
  );
}

static inline void cswap2(uint8_t bit, uint64_t *const p0, uint64_t *const p1) {
  cswap1(bit,p0,p1);
  cswap1(bit,p0+4,p1+4);
}


inline static void Hacl_Impl_Curve25519_Field64_store_felem(uint64_t *u64s, uint64_t *f)
{
  uint64_t f3 = f[3U];
  uint64_t top_bit = f3 >> (uint32_t)63U;
  uint64_t carry1;
  uint64_t f31;
  uint64_t top_bit1;
  uint64_t carry2;
  f[3U] = f3 & (uint64_t)0x7fffffffffffffffU;
  carry1 = add1(f, f, (uint64_t)19U * top_bit);
  f31 = f[3U];
  top_bit1 = f31 >> (uint32_t)63U;
  f[3U] = f31 & (uint64_t)0x7fffffffffffffffU;
  carry2 = add1(f, f, (uint64_t)19U * top_bit1);
  u64s[0U] = f[0U];
  u64s[1U] = f[1U];
  u64s[2U] = f[2U];
  u64s[3U] = f[3U];
}

inline static void
Hacl_Impl_Curve25519_Generic_fsquare_times_64(
  uint64_t *o,
  uint64_t *i,
  uint64_t *tmp,
  uint32_t n1
)
{
  uint32_t i0;
  fsqr(o, i, (uint64_t *)tmp);
  for (i0 = (uint32_t)0U; i0 < n1 - (uint32_t)1U; i0 = i0 + (uint32_t)1U)
  {
    fsqr(o, o, (uint64_t *)tmp);
  }
}

inline static void Hacl_Impl_Curve25519_Generic_finv_64(uint64_t *o, uint64_t *i, uint64_t *tmp)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t t1[(uint32_t)4U * (uint32_t)4U];
    memset(t1, 0U, (uint32_t)4U * (uint32_t)4U * sizeof t1[0U]);
    {
      uint64_t *a = (uint64_t *)t1;
      uint64_t *b = (uint64_t *)(t1 + (uint32_t)4U);
      uint64_t *c = (uint64_t *)(t1 + (uint32_t)2U * (uint32_t)4U);
      uint64_t *t0 = (uint64_t *)(t1 + (uint32_t)3U * (uint32_t)4U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)a, i, tmp, (uint32_t)1U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)a,
        tmp,
        (uint32_t)2U);
      fmul((uint64_t *)b, (uint64_t *)t0, i, (uint64_t *)tmp);
      fmul((uint64_t *)a, (uint64_t *)b, (uint64_t *)a, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)a,
        tmp,
        (uint32_t)1U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)5U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)10U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)c,
        tmp,
        (uint32_t)20U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)10U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)50U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)c,
        tmp,
        (uint32_t)100U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)50U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)5U);
      fmul(o, (uint64_t *)t0, (uint64_t *)a, (uint64_t *)tmp);
    }
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_point_add_and_double_64(
  uint64_t *q,
  uint64_t *nq,
  uint64_t *nq_p1,
  uint64_t *tmp1,
  uint64_t *tmp2
)
{
  uint64_t *x1 = q;
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  uint64_t *x3 = nq_p1;
  uint64_t *z3 = nq_p1 + (uint32_t)4U;
  uint64_t *a = (uint64_t *)tmp1;
  uint64_t *b = (uint64_t *)(tmp1 + (uint32_t)4U);
  uint64_t *d = (uint64_t *)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  uint64_t *c = (uint64_t *)(tmp1 + (uint32_t)3U * (uint32_t)4U);
  uint64_t *ab = (uint64_t *)tmp1;
  uint64_t *dc = (uint64_t *)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  fadd((uint64_t *)a, x2, z2);
  fsub((uint64_t *)b, x2, z2);
  fadd((uint64_t *)c, x3, z3);
  fsub((uint64_t *)d, x3, z3);
  fmul2((uint64_t *)dc, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  fadd(x3, (uint64_t *)d, (uint64_t *)c);
  fsub(z3, (uint64_t *)d, (uint64_t *)c);
  fsqr2((uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  fsqr2(nq_p1, nq_p1, (uint64_t *)tmp2);
  ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
  ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
  ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
  ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
  fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
  fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
  fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
  fmul2(nq, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  fmul(z3, z3, x1, (uint64_t *)tmp2);
}

inline static void
Hacl_Impl_Curve25519_Generic_point_double_64(uint64_t *nq, uint64_t *tmp1, uint64_t *tmp2)
{
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  uint64_t *a = (uint64_t *)tmp1;
  uint64_t *b = (uint64_t *)(tmp1 + (uint32_t)4U);
  uint64_t *d = (uint64_t *)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  uint64_t *c = (uint64_t *)(tmp1 + (uint32_t)3U * (uint32_t)4U);
  uint64_t *ab = (uint64_t *)tmp1;
  uint64_t *dc = (uint64_t *)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  fadd((uint64_t *)a, x2, z2);
  fsub((uint64_t *)b, x2, z2);
  fsqr2((uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
  ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
  ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
  ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
  fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
  fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
  fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
  fmul2(nq, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
}

inline static void
Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(
  uint64_t *out,
  uint64_t *key,
  uint64_t *init1
)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t p01[(uint32_t)4U * (uint32_t)4U];
    memset(p01, 0U, (uint32_t)4U * (uint32_t)4U * sizeof p01[0U]);
    {
      uint64_t *p0 = p01;
      uint64_t *p1 = p01 + (uint32_t)2U * (uint32_t)4U;
      uint64_t *x0;
      uint64_t swap1[1U];
      memcpy(p1, init1, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      x0 = (uint64_t *)p0;
      ((uint64_t *)x0)[0U] = (uint64_t)1U;
      ((uint64_t *)x0)[1U] = (uint64_t)0U;
      ((uint64_t *)x0)[2U] = (uint64_t)0U;
      ((uint64_t *)x0)[3U] = (uint64_t)0U;
      swap1[0U] = (uint64_t)0U;
      KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
      {
        uint64_t tmp1[(uint32_t)4U * (uint32_t)4U];
        memset(tmp1, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp1[0U]);
        KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
        {
          uint64_t tmp2[(uint32_t)2U * (uint32_t)8U];
          memset(tmp2, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp2[0U]);
          {
            uint32_t i;
            for (i = (uint32_t)0U; i < (uint32_t)252U; i = i + (uint32_t)1U)
            {
              uint64_t
              bit =
                key[((uint32_t)254U - i)
                / (uint32_t)64U]
                >> ((uint32_t)254U - i) % (uint32_t)64U
                & (uint64_t)1U;
              uint64_t sw = swap1[0U] ^ bit;
              cswap2(sw, p0, p1);
              Hacl_Impl_Curve25519_Generic_point_add_and_double_64(init1,
                p0,
                p1,
                tmp1,
                (uint64_t *)tmp2);
              swap1[0U] = bit;
            }
          }
          cswap2(swap1[0U], p0, p1);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (uint64_t *)tmp2);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (uint64_t *)tmp2);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (uint64_t *)tmp2);
          memcpy(out, p0, (uint32_t)2U * (uint32_t)4U * sizeof p0[0U]);
        }
      }
    }
  }
}

static uint8_t
Hacl_Impl_Curve25519_Generic_g25519[32U] =
  {
    (uint8_t)9U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U
  };

void Hacl_Curve25519_64_secret_to_public(uint8_t *pub, uint8_t *priv)
{
  uint8_t basepoint[32U] = { 0U };
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)32U; i = i + (uint32_t)1U)
    {
      basepoint[i] = Hacl_Impl_Curve25519_Generic_g25519[i];
    }
  }
  {
    uint64_t scalar[4U] = { 0U };
    {
      uint8_t *b_i = priv + (uint32_t)0U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[0U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)1U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[1U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)2U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[2U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)3U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[3U] = u_i;
    }
    scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
    scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
    scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
    KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
    {
      uint64_t init1[(uint32_t)2U * (uint32_t)4U];
      memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      {
        uint64_t tmp[4U] = { 0U };
        uint64_t *x;
        uint64_t *z0;
        {
          uint8_t *b_i = basepoint + (uint32_t)0U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[0U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)1U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[1U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)2U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[2U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)3U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[3U] = u_i;
        }
        tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
        x = (uint64_t *)init1;
        z0 = (uint64_t *)(init1 + (uint32_t)4U);
        ((uint64_t *)z0)[0U] = (uint64_t)1U;
        ((uint64_t *)z0)[1U] = (uint64_t)0U;
        ((uint64_t *)z0)[2U] = (uint64_t)0U;
        ((uint64_t *)z0)[3U] = (uint64_t)0U;
        ((uint64_t *)x)[0U] = tmp[0U];
        ((uint64_t *)x)[1U] = tmp[1U];
        ((uint64_t *)x)[2U] = tmp[2U];
        ((uint64_t *)x)[3U] = tmp[3U];
        Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
        {
          uint64_t *x0 = (uint64_t *)init1;
          uint64_t *z = (uint64_t *)(init1 + (uint32_t)4U);
          uint64_t buf[4U] = { 0U };
          uint64_t *tmp0 = (uint64_t *)buf;
          uint64_t u64s[4U] = { 0U };
          KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
          {
            uint64_t tmp_w[(uint32_t)2U * (uint32_t)8U];
            memset(tmp_w, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp_w[0U]);
            Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp0, (uint64_t *)z, (uint64_t *)tmp_w);
            fmul((uint64_t *)tmp0, (uint64_t *)tmp0, (uint64_t *)x0, tmp_w);
            Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp0);
            {
              uint64_t u_i = u64s[0U];
              uint8_t *b_i = pub + (uint32_t)0U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[1U];
              uint8_t *b_i = pub + (uint32_t)1U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[2U];
              uint8_t *b_i = pub + (uint32_t)2U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[3U];
              uint8_t *b_i = pub + (uint32_t)3U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
          }
        }
      }
    }
  }
}

void curve25519_mixed64(uint8_t *shared, uint8_t *my_priv, uint8_t *their_pub)
{
  uint64_t scalar[4U] = { 0U };
  {
    uint8_t *b_i = my_priv + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[0U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[1U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[2U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[3U] = u_i;
  }
  scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
  scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
  scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
  {
    uint64_t init1[(uint32_t)2U * (uint32_t)4U];
    memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
    {
      uint64_t tmp[4U] = { 0U };
      uint64_t *x;
      uint64_t *z0;
      {
        uint8_t *b_i = their_pub + (uint32_t)0U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[0U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)1U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[1U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)2U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[2U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)3U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[3U] = u_i;
      }
      tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
      x = (uint64_t *)init1;
      z0 = (uint64_t *)(init1 + (uint32_t)4U);
      ((uint64_t *)z0)[0U] = (uint64_t)1U;
      ((uint64_t *)z0)[1U] = (uint64_t)0U;
      ((uint64_t *)z0)[2U] = (uint64_t)0U;
      ((uint64_t *)z0)[3U] = (uint64_t)0U;
      ((uint64_t *)x)[0U] = tmp[0U];
      ((uint64_t *)x)[1U] = tmp[1U];
      ((uint64_t *)x)[2U] = tmp[2U];
      ((uint64_t *)x)[3U] = tmp[3U];
      Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
      {
        uint64_t *x0 = (uint64_t *)init1;
        uint64_t *z = (uint64_t *)(init1 + (uint32_t)4U);
        uint64_t buf[4U] = { 0U };
        uint64_t *tmp0 = (uint64_t *)buf;
        uint64_t u64s[4U] = { 0U };
        KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
        {
          uint64_t tmp_w[(uint32_t)2U * (uint32_t)8U];
          memset(tmp_w, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp_w[0U]);
          Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp0, (uint64_t *)z, (uint64_t *)tmp_w);
          fmul((uint64_t *)tmp0, (uint64_t *)tmp0, (uint64_t *)x0, tmp_w);
          Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp0);
          {
            uint64_t u_i = u64s[0U];
            uint8_t *b_i = shared + (uint32_t)0U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[1U];
            uint8_t *b_i = shared + (uint32_t)1U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[2U];
            uint8_t *b_i = shared + (uint32_t)2U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[3U];
            uint8_t *b_i = shared + (uint32_t)3U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
        }
      }
    }
  }
}

