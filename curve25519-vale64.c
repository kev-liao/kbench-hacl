/* This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
 * KreMLin invocation: /home/bhargava/Desktop/repositories/kremlin/krml -funroll-loops 8 -warn-error +9 -I /home/bhargava/Desktop/repositories/hacl-star/lib/ -I /home/bhargava/Desktop/repositories/hacl-star/lib/fst -I /home/bhargava/Desktop/repositories/kremlin/kremlib -I /home/bhargava/Desktop/repositories/hacl-star/specs -I ./lemmas -I . -ccopt -march=native -fbuiltin-uint128 -drop FStar.UInt128 -fnocompound-literals -fc89-scope -fparentheses -fcurly-braces -tmpdir curve64-c curve64-c/out.krml -skip-compilation -minimal -add-include "kremlib.h" -bundle Hacl.Curve25519_64=* -drop Hacl.Impl.Curve25519.Field64.Core -no-prefix Hacl.Impl.Curve25519.Field64.Core -add-include "vale_25519.h"
 * F* version: 1014d13d
 * KreMLin version: d0fcbc62
 */

#include <linux/kernel.h>
#include <linux/string.h>
enum { CURVE25519_POINT_SIZE = 32 };
typedef __uint128_t u128;
typedef uint8_t u8;
typedef uint32_t u32;
typedef uint64_t u64;

#define load64_le(b_i) le64_to_cpup((__force __le64 *)b_i)
#define store64_le(b_o,o) *(__force __le64 *)b_o = cpu_to_le64(o)

#define inline __always_inline

#define KRML_CHECK_SIZE(a,b) {}

asmlinkage void vale_fmul_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst, const uint64_t* in_b);
asmlinkage void vale_fmul2_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst, const uint64_t* in_b);
asmlinkage void vale_fsqr_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst);
asmlinkage void vale_fsqr2_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst);
asmlinkage void vale_carry_wide(uint64_t* dst, uint64_t* tmp);
asmlinkage void vale_fadd(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b);
asmlinkage void vale_fsub(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b);
asmlinkage void vale_fmul1(uint64_t* dst, const uint64_t* in_a, const uint64_t in_b);
asmlinkage uint64_t vale_add1(const uint64_t* dst, const uint64_t* in_a, uint64_t b);

static inline
void fmul(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b, uint64_t* tmp) {
  vale_fmul_v(tmp, in_a, dst, in_b);
}

// Done in C in rfc7748_25519.h
static inline
void fsqr(uint64_t* dst, const uint64_t* in_a, uint64_t* tmp) {
  vale_fsqr_v(tmp,in_a, dst);
}

// Done in C in rfc7748_25519.h
static inline
void fmul2(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b, uint64_t* tmp) {
  vale_fmul2_v(tmp, in_a, dst, in_b);
}

// Done in C in rfc7748_25519.h
static inline
void fsqr2(uint64_t* dst, const uint64_t* in_a, uint64_t* tmp) {
  vale_fsqr2_v(tmp, in_a, dst);
}


static inline void cswap1(uint8_t bit, uint64_t *const p0, uint64_t *const p1) {
  uint64_t temp;
  __asm__ __volatile__(
    "test %9, %9 ;"
    "movq %0, %8 ;"
    "cmovnzq %4, %0 ;"
    "cmovnzq %8, %4 ;"
    "movq %1, %8 ;"
    "cmovnzq %5, %1 ;"
    "cmovnzq %8, %5 ;"
    "movq %2, %8 ;"
    "cmovnzq %6, %2 ;"
    "cmovnzq %8, %6 ;"
    "movq %3, %8 ;"
    "cmovnzq %7, %3 ;"
    "cmovnzq %8, %7 ;"
    : "+r"(p0[0]), "+r"(p0[1]), "+r"(p0[2]), "+r"(p0[3]),
      "+r"(p1[0]), "+r"(p1[1]), "+r"(p1[2]), "+r"(p1[3]),
      "=r"(temp)
    : "r"(bit)
    : "cc"
  );
}

static inline void cswap2(uint8_t bit, uint64_t *const p0, uint64_t *const p1) {
  cswap1(bit,p0,p1);
  cswap1(bit,p0+4,p1+4);
}


inline static void Hacl_Impl_Curve25519_Field64_store_felem(uint64_t *u64s, uint64_t *f)
{
  uint64_t f3 = f[3U];
  uint64_t top_bit = f3 >> (uint32_t)63U;
  uint64_t carry1;
  uint64_t f31;
  uint64_t top_bit1;
  uint64_t carry2;
  f[3U] = f3 & (uint64_t)0x7fffffffffffffffU;
  carry1 = vale_add1(f, f, (uint64_t)19U * top_bit);
  f31 = f[3U];
  top_bit1 = f31 >> (uint32_t)63U;
  f[3U] = f31 & (uint64_t)0x7fffffffffffffffU;
  carry2 = vale_add1(f, f, (uint64_t)19U * top_bit1);
  u64s[0U] = f[0U];
  u64s[1U] = f[1U];
  u64s[2U] = f[2U];
  u64s[3U] = f[3U];
}

inline static void
Hacl_Impl_Curve25519_Generic_fsquare_times_64(
  uint64_t *o,
  uint64_t *i,
  void **tmp,
  uint32_t n1
)
{
  uint32_t i0;
  fsqr(o, i, (uint64_t *)tmp);
  for (i0 = (uint32_t)0U; i0 < n1 - (uint32_t)1U; i0 = i0 + (uint32_t)1U)
  {
    fsqr(o, o, (uint64_t *)tmp);
  }
}

inline static void Hacl_Impl_Curve25519_Generic_finv_64(uint64_t *o, uint64_t *i, void **tmp)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t t1[(uint32_t)4U * (uint32_t)4U];
    memset(t1, 0U, (uint32_t)4U * (uint32_t)4U * sizeof t1[0U]);
    {
      void **a = (void **)t1;
      void **b = (void **)(t1 + (uint32_t)4U);
      void **c = (void **)(t1 + (uint32_t)2U * (uint32_t)4U);
      void **t0 = (void **)(t1 + (uint32_t)3U * (uint32_t)4U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)a, i, tmp, (uint32_t)1U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)a,
        tmp,
        (uint32_t)2U);
      fmul((uint64_t *)b, (uint64_t *)t0, i, (uint64_t *)tmp);
      fmul((uint64_t *)a, (uint64_t *)b, (uint64_t *)a, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)a,
        tmp,
        (uint32_t)1U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)5U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)10U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)c,
        tmp,
        (uint32_t)20U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)10U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)b,
        tmp,
        (uint32_t)50U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)c,
        tmp,
        (uint32_t)100U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)50U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)b, (uint64_t *)tmp);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0,
        (uint64_t *)t0,
        tmp,
        (uint32_t)5U);
      fmul(o, (uint64_t *)t0, (uint64_t *)a, (uint64_t *)tmp);
    }
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_point_add_and_double_64(
  uint64_t *q,
  uint64_t *nq,
  uint64_t *nq_p1,
  uint64_t *tmp1,
  void **tmp2
)
{
  uint64_t *x1 = q;
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  uint64_t *x3 = nq_p1;
  uint64_t *z3 = nq_p1 + (uint32_t)4U;
  void **a = (void **)tmp1;
  void **b = (void **)(tmp1 + (uint32_t)4U);
  void **d = (void **)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  void **c = (void **)(tmp1 + (uint32_t)3U * (uint32_t)4U);
  void **ab = (void **)tmp1;
  void **dc = (void **)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  vale_fadd((uint64_t *)a, x2, z2);
  vale_fsub((uint64_t *)b, x2, z2);
  vale_fadd((uint64_t *)c, x3, z3);
  vale_fsub((uint64_t *)d, x3, z3);
  fmul2((uint64_t *)dc, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  vale_fadd(x3, (uint64_t *)d, (uint64_t *)c);
  vale_fsub(z3, (uint64_t *)d, (uint64_t *)c);
  fsqr2((uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  fsqr2(nq_p1, nq_p1, (uint64_t *)tmp2);
  ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
  ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
  ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
  ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
  vale_fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
  vale_fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
  vale_fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
  fmul2(nq, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  fmul(z3, z3, x1, (uint64_t *)tmp2);
}

inline static void
Hacl_Impl_Curve25519_Generic_point_double_64(uint64_t *nq, uint64_t *tmp1, void **tmp2)
{
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  void **a = (void **)tmp1;
  void **b = (void **)(tmp1 + (uint32_t)4U);
  void **d = (void **)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  void **c = (void **)(tmp1 + (uint32_t)3U * (uint32_t)4U);
  void **ab = (void **)tmp1;
  void **dc = (void **)(tmp1 + (uint32_t)2U * (uint32_t)4U);
  vale_fadd((uint64_t *)a, x2, z2);
  vale_fsub((uint64_t *)b, x2, z2);
  fsqr2((uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
  ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
  ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
  ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
  ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
  vale_fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
  vale_fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
  vale_fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
  fmul2(nq, (uint64_t *)dc, (uint64_t *)ab, (uint64_t *)tmp2);
}

inline static void
Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(
  uint64_t *out,
  uint64_t *key,
  uint64_t *init1
)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t p01[(uint32_t)4U * (uint32_t)4U];
    memset(p01, 0U, (uint32_t)4U * (uint32_t)4U * sizeof p01[0U]);
    {
      uint64_t *p0 = p01;
      uint64_t *p1 = p01 + (uint32_t)2U * (uint32_t)4U;
      void **x0;
      uint64_t swap1[1U];
      memcpy(p1, init1, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      x0 = (void **)p0;
      ((uint64_t *)x0)[0U] = (uint64_t)1U;
      ((uint64_t *)x0)[1U] = (uint64_t)0U;
      ((uint64_t *)x0)[2U] = (uint64_t)0U;
      ((uint64_t *)x0)[3U] = (uint64_t)0U;
      swap1[0U] = (uint64_t)0U;
      KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
      {
        uint64_t tmp1[(uint32_t)4U * (uint32_t)4U];
        memset(tmp1, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp1[0U]);
        KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
        {
          uint64_t tmp2[(uint32_t)2U * (uint32_t)8U];
          memset(tmp2, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp2[0U]);
          {
            uint32_t i;
            for (i = (uint32_t)0U; i < (uint32_t)252U; i = i + (uint32_t)1U)
            {
              uint64_t
              bit =
                key[((uint32_t)254U - i)
                / (uint32_t)64U]
                >> ((uint32_t)254U - i) % (uint32_t)64U
                & (uint64_t)1U;
              uint64_t sw = swap1[0U] ^ bit;
              cswap2(sw, p0, p1);
              Hacl_Impl_Curve25519_Generic_point_add_and_double_64(init1,
                p0,
                p1,
                tmp1,
                (void **)tmp2);
              swap1[0U] = bit;
            }
          }
          cswap2(swap1[0U], p0, p1);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (void **)tmp2);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (void **)tmp2);
          Hacl_Impl_Curve25519_Generic_point_double_64(p0, tmp1, (void **)tmp2);
          memcpy(out, p0, (uint32_t)2U * (uint32_t)4U * sizeof p0[0U]);
        }
      }
    }
  }
}

static uint8_t
Hacl_Impl_Curve25519_Generic_g25519[32U] =
  {
    (uint8_t)9U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U
  };

void Hacl_Curve25519_64_secret_to_public_(uint8_t *pub, uint8_t *priv)
{
  uint8_t basepoint[32U] = { 0U };
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)32U; i = i + (uint32_t)1U)
    {
      basepoint[i] = Hacl_Impl_Curve25519_Generic_g25519[i];
    }
  }
  {
    uint64_t scalar[4U] = { 0U };
    {
      uint8_t *b_i = priv + (uint32_t)0U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[0U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)1U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[1U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)2U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[2U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)3U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[3U] = u_i;
    }
    scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
    scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
    scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
    KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
    {
      uint64_t init1[(uint32_t)2U * (uint32_t)4U];
      memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      {
        uint64_t tmp[4U] = { 0U };
        void **x;
        void **z0;
        {
          uint8_t *b_i = basepoint + (uint32_t)0U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[0U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)1U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[1U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)2U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[2U] = u_i;
        }
        {
          uint8_t *b_i = basepoint + (uint32_t)3U * (uint32_t)8U;
          uint64_t u = load64_le(b_i);
          uint64_t u_i = u;
          tmp[3U] = u_i;
        }
        tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
        x = (void **)init1;
        z0 = (void **)(init1 + (uint32_t)4U);
        ((uint64_t *)z0)[0U] = (uint64_t)1U;
        ((uint64_t *)z0)[1U] = (uint64_t)0U;
        ((uint64_t *)z0)[2U] = (uint64_t)0U;
        ((uint64_t *)z0)[3U] = (uint64_t)0U;
        ((uint64_t *)x)[0U] = tmp[0U];
        ((uint64_t *)x)[1U] = tmp[1U];
        ((uint64_t *)x)[2U] = tmp[2U];
        ((uint64_t *)x)[3U] = tmp[3U];
        Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
        {
          void **x0 = (void **)init1;
          void **z = (void **)(init1 + (uint32_t)4U);
          uint64_t buf[4U] = { 0U };
          void **tmp0 = (void **)buf;
          uint64_t u64s[4U] = { 0U };
          KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
          {
            uint64_t tmp_w[(uint32_t)2U * (uint32_t)8U];
            memset(tmp_w, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp_w[0U]);
            Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp0, (uint64_t *)z, (void **)tmp_w);
            fmul((uint64_t *)tmp0, (uint64_t *)tmp0, (uint64_t *)x0, tmp_w);
            Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp0);
            {
              uint64_t u_i = u64s[0U];
              uint8_t *b_i = pub + (uint32_t)0U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[1U];
              uint8_t *b_i = pub + (uint32_t)1U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[2U];
              uint8_t *b_i = pub + (uint32_t)2U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
            {
              uint64_t u_i = u64s[3U];
              uint8_t *b_i = pub + (uint32_t)3U * (uint32_t)8U;
              store64_le(b_i, u_i);
            }
          }
        }
      }
    }
  }
}

void curve25519_vale64(uint8_t *shared, uint8_t *my_priv, uint8_t *their_pub)
{
  uint64_t scalar[4U] = { 0U };
  {
    uint8_t *b_i = my_priv + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[0U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[1U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[2U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[3U] = u_i;
  }
  scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
  scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
  scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
  {
    uint64_t init1[(uint32_t)2U * (uint32_t)4U];
    memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
    {
      uint64_t tmp[4U] = { 0U };
      void **x;
      void **z0;
      {
        uint8_t *b_i = their_pub + (uint32_t)0U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[0U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)1U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[1U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)2U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[2U] = u_i;
      }
      {
        uint8_t *b_i = their_pub + (uint32_t)3U * (uint32_t)8U;
        uint64_t u = load64_le(b_i);
        uint64_t u_i = u;
        tmp[3U] = u_i;
      }
      tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
      x = (void **)init1;
      z0 = (void **)(init1 + (uint32_t)4U);
      ((uint64_t *)z0)[0U] = (uint64_t)1U;
      ((uint64_t *)z0)[1U] = (uint64_t)0U;
      ((uint64_t *)z0)[2U] = (uint64_t)0U;
      ((uint64_t *)z0)[3U] = (uint64_t)0U;
      ((uint64_t *)x)[0U] = tmp[0U];
      ((uint64_t *)x)[1U] = tmp[1U];
      ((uint64_t *)x)[2U] = tmp[2U];
      ((uint64_t *)x)[3U] = tmp[3U];
      Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
      {
        void **x0 = (void **)init1;
        void **z = (void **)(init1 + (uint32_t)4U);
        uint64_t buf[4U] = { 0U };
        void **tmp0 = (void **)buf;
        uint64_t u64s[4U] = { 0U };
        KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)8U);
        {
          uint64_t tmp_w[(uint32_t)2U * (uint32_t)8U];
          memset(tmp_w, 0U, (uint32_t)2U * (uint32_t)8U * sizeof tmp_w[0U]);
          Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp0, (uint64_t *)z, (void **)tmp_w);
          fmul((uint64_t *)tmp0, (uint64_t *)tmp0, (uint64_t *)x0, tmp_w);
          Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp0);
          {
            uint64_t u_i = u64s[0U];
            uint8_t *b_i = shared + (uint32_t)0U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[1U];
            uint8_t *b_i = shared + (uint32_t)1U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[2U];
            uint8_t *b_i = shared + (uint32_t)2U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
          {
            uint64_t u_i = u64s[3U];
            uint8_t *b_i = shared + (uint32_t)3U * (uint32_t)8U;
            store64_le(b_i, u_i);
          }
        }
      }
    }
  }
}

