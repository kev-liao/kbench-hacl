/* This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
 * KreMLin invocation: /home/bhargava/Desktop/repositories/kremlin/krml -funroll-loops 8 -warn-error +9 -I /home/bhargava/Desktop/repositories/hacl-star/lib/ -I /home/bhargava/Desktop/repositories/hacl-star/lib/fst -I /home/bhargava/Desktop/repositories/kremlin/kremlib -I /home/bhargava/Desktop/repositories/hacl-star/specs -I ./lemmas -I . -ccopt -march=native -fbuiltin-uint128 -drop FStar.UInt128 -fnocompound-literals -fc89-scope -fparentheses -fcurly-braces -tmpdir curve64-c curve64-c/out.krml -skip-compilation -minimal -add-include "kremlib.h" -bundle Hacl.Curve25519_64=* -drop Hacl.Impl.Curve25519.Field64.Core -no-prefix Hacl.Impl.Curve25519.Field64.Core -add-include "vale_25519.h"
 * F* version: 1014d13d
 * KreMLin version: d0fcbc62
 */

#include <linux/kernel.h>
#include <linux/string.h>
enum { CURVE25519_POINT_SIZE = 32 };
typedef __uint128_t u128;
typedef uint8_t u8;
typedef uint32_t u32;
typedef uint64_t u64;

#define load64_le(b_i) le64_to_cpup((__force __le64 *)b_i)
#define store64_le(b_o,o) *(__force __le64 *)b_o = cpu_to_le64(o)

#define inline __always_inline


asmlinkage void vale_fmul_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst, const uint64_t* in_b);
asmlinkage void vale_fmul2_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst, const uint64_t* in_b);
asmlinkage void vale_fsqr_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst);
asmlinkage void vale_fsqr2_v(const uint64_t* tmp, const uint64_t* in_a, const uint64_t* dst);
asmlinkage void vale_carry_wide(uint64_t* dst, uint64_t* tmp);
asmlinkage void vale_fadd(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b);
asmlinkage void vale_fsub(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b);
asmlinkage void vale_fmul1(uint64_t* dst, const uint64_t* in_a, const uint64_t in_b);
asmlinkage uint64_t vale_add1(const uint64_t* dst, const uint64_t* in_a, uint64_t b);

// Done in C in rfc7748_25519.h
static inline
void fmul(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b) {
  uint64_t tmp[8] = {0};
  vale_fmul_v(tmp, in_a, dst, in_b);
}

// Done in C in rfc7748_25519.h
static inline 
void fsqr(uint64_t* dst, const uint64_t* in_a) {
  uint64_t tmp[8] = {0};
  vale_fsqr_v(tmp,in_a, dst);
}

// Done in C in rfc7748_25519.h
static inline
void fmul2(uint64_t* dst, const uint64_t* in_a, const uint64_t* in_b) {
  uint64_t tmp[16] = {0};
  vale_fmul2_v(tmp, in_a, dst, in_b);
}

// Done in C in rfc7748_25519.h
static inline 
void fsqr2(uint64_t* dst, const uint64_t* in_a) {
  uint64_t tmp[16] = {0};
  vale_fsqr2_v(tmp, in_a, dst);
}


inline static void Hacl_Impl_Curve25519_Field64_store_felem(uint64_t *u64s, uint64_t *f)
{
  uint64_t f3 = f[3U];
  uint64_t top_bit = f3 >> (uint32_t)63U;
  uint64_t carry1;
  uint64_t f31;
  uint64_t top_bit1;
  uint64_t carry2;
  f[3U] = f3 & (uint64_t)0x7fffffffffffffffU;
  carry1 = vale_add1(f, f, (uint64_t)19U * top_bit);
  f31 = f[3U];
  top_bit1 = f31 >> (uint32_t)63U;
  f[3U] = f31 & (uint64_t)0x7fffffffffffffffU;
  carry2 = vale_add1(f, f, (uint64_t)19U * top_bit1);
  u64s[0U] = f[0U];
  u64s[1U] = f[1U];
  u64s[2U] = f[2U];
  u64s[3U] = f[3U];
}

inline static void
Hacl_Impl_Curve25519_Generic_fsquare_times_64(uint64_t *o, uint64_t *i, uint32_t n1)
{
  uint32_t i0;
  fsqr(o, i);
  for (i0 = (uint32_t)0U; i0 < n1 - (uint32_t)1U; i0 = i0 + (uint32_t)1U)
  {
    fsqr(o, o);
  }
}

static void Hacl_Impl_Curve25519_Generic_finv_64(uint64_t *o, uint64_t *i)
{
  {
    uint64_t tmp[(uint32_t)4U * (uint32_t)4U];
    memset(tmp, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp[0U]);
    {
      void **a = (void **)tmp;
      void **b = (void **)(tmp + (uint32_t)4U);
      void **c = (void **)(tmp + (uint32_t)2U * (uint32_t)4U);
      void **t0 = (void **)(tmp + (uint32_t)3U * (uint32_t)4U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)a, i, (uint32_t)1U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)a, (uint32_t)2U);
      fmul((uint64_t *)b, (uint64_t *)t0, i);
      fmul((uint64_t *)a, (uint64_t *)b, (uint64_t *)a);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)a, (uint32_t)1U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)5U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)10U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)c, (uint32_t)20U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)10U);
      fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)50U);
      fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)c, (uint32_t)100U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)50U);
      fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)5U);
      fmul(o, (uint64_t *)t0, (uint64_t *)a);
    }
  }
}

inline static void Hacl_Impl_Curve25519_Generic_decode_point_64(uint64_t *o, uint8_t *i)
{
  uint64_t tmp[4U] = { 0U };
  void **x;
  void **z;
  {
    uint8_t *b_i = i + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[0U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[1U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[2U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[3U] = u_i;
  }
  tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
  x = (void **)o;
  z = (void **)(o + (uint32_t)4U);
  ((uint64_t *)z)[0U] = (uint64_t)0U;
  ((uint64_t *)z)[1U] = (uint64_t)0U;
  ((uint64_t *)z)[2U] = (uint64_t)0U;
  ((uint64_t *)z)[3U] = (uint64_t)0U;
  ((uint64_t *)z)[0U] = ((uint64_t *)z)[0U] | (uint64_t)1U;
  ((uint64_t *)x)[0U] = tmp[0U];
  ((uint64_t *)x)[1U] = tmp[1U];
  ((uint64_t *)x)[2U] = tmp[2U];
  ((uint64_t *)x)[3U] = tmp[3U];
}

inline static void Hacl_Impl_Curve25519_Generic_encode_point_64(uint8_t *o, uint64_t *i)
{
  void **x = (void **)i;
  void **z = (void **)(i + (uint32_t)4U);
  uint64_t buf[4U] = { 0U };
  void **tmp = (void **)buf;
  uint64_t u64s[4U] = { 0U };
  Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp, (uint64_t *)z);
  fmul((uint64_t *)tmp, (uint64_t *)tmp, (uint64_t *)x);
  Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp);
  {
    uint64_t u_i = u64s[0U];
    uint8_t *b_i = o + (uint32_t)0U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[1U];
    uint8_t *b_i = o + (uint32_t)1U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[2U];
    uint8_t *b_i = o + (uint32_t)2U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[3U];
    uint8_t *b_i = o + (uint32_t)3U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_point_add_and_double_64(
  uint64_t *q,
  uint64_t *nq,
  uint64_t *nq_p1
)
{
  uint64_t *x1 = q;
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  uint64_t *x3 = nq_p1;
  uint64_t *z3 = nq_p1 + (uint32_t)4U;
  {
    uint64_t tmp[(uint32_t)4U * (uint32_t)4U];
    memset(tmp, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp[0U]);
    {
      void **a = (void **)tmp;
      void **b = (void **)(tmp + (uint32_t)4U);
      void **d = (void **)(tmp + (uint32_t)2U * (uint32_t)4U);
      void **c = (void **)(tmp + (uint32_t)3U * (uint32_t)4U);
      vale_fadd((uint64_t *)a, x2, z2);
      vale_fsub((uint64_t *)b, x2, z2);
      vale_fadd((uint64_t *)c, x3, z3);
      vale_fsub((uint64_t *)d, x3, z3);
      fmul2((uint64_t *)d, (uint64_t *)d, (uint64_t *)a);
      vale_fadd(x3, (uint64_t *)d, (uint64_t *)c);
      vale_fsub(z3, (uint64_t *)d, (uint64_t *)c);
      fsqr2((uint64_t *)d, (uint64_t *)a);
      fsqr2(x3, x3);
      ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
      ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
      ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
      ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
      vale_fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
      vale_fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
      vale_fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
      fmul2(x2, (uint64_t *)d, (uint64_t *)a);
      fmul(z3, z3, x1);
    }
  }
}

inline static void Hacl_Impl_Curve25519_Generic_point_double_64(uint64_t *nq)
{
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  {
    uint64_t tmp[(uint32_t)4U * (uint32_t)4U];
    memset(tmp, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp[0U]);
    {
      void **a = (void **)tmp;
      void **b = (void **)(tmp + (uint32_t)4U);
      void **d = (void **)(tmp + (uint32_t)2U * (uint32_t)4U);
      void **c = (void **)(tmp + (uint32_t)3U * (uint32_t)4U);
      vale_fadd((uint64_t *)a, x2, z2);
      vale_fsub((uint64_t *)b, x2, z2);
      fsqr2((uint64_t *)d, (uint64_t *)a);
      ((uint64_t *)a)[0U] = ((uint64_t *)c)[0U];
      ((uint64_t *)a)[1U] = ((uint64_t *)c)[1U];
      ((uint64_t *)a)[2U] = ((uint64_t *)c)[2U];
      ((uint64_t *)a)[3U] = ((uint64_t *)c)[3U];
      vale_fsub((uint64_t *)c, (uint64_t *)d, (uint64_t *)c);
      vale_fmul1((uint64_t *)b, (uint64_t *)c, (uint64_t)121665U);
      vale_fadd((uint64_t *)b, (uint64_t *)b, (uint64_t *)d);
      fmul2(x2, (uint64_t *)d, (uint64_t *)a);
    }
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_cswap_64(uint64_t bit, uint64_t *p0, uint64_t *p1)
{
  uint64_t mask = (uint64_t)0U - bit;
  uint32_t i;
  for (i = (uint32_t)0U; i < (uint32_t)2U * (uint32_t)4U; i = i + (uint32_t)1U)
  {
    uint64_t dummy = mask & (p0[i] ^ p1[i]);
    p0[i] = p0[i] ^ dummy;
    p1[i] = p1[i] ^ dummy;
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(
  uint64_t *out,
  uint64_t *key,
  uint64_t *init1
)
{
  {
    uint64_t p01[(uint32_t)4U * (uint32_t)4U];
    memset(p01, 0U, (uint32_t)4U * (uint32_t)4U * sizeof p01[0U]);
    {
      uint64_t *p0 = p01;
      uint64_t *p1 = p01 + (uint32_t)2U * (uint32_t)4U;
      void **x0;
      uint64_t swap1[1U];
      memcpy(p1, init1, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      x0 = (void **)p0;
      ((uint64_t *)x0)[0U] = ((uint64_t *)x0)[0U] | (uint64_t)1U;
      swap1[0U] = (uint64_t)0U;
      {
        uint32_t i;
        for (i = (uint32_t)0U; i < (uint32_t)252U; i = i + (uint32_t)1U)
        {
          uint64_t
          bit =
            key[((uint32_t)254U - i)
            / (uint32_t)64U]
            >> ((uint32_t)254U - i) % (uint32_t)64U
            & (uint64_t)1U;
          uint64_t sw = swap1[0U] ^ bit;
          Hacl_Impl_Curve25519_Generic_cswap_64(sw, p0, p1);
          Hacl_Impl_Curve25519_Generic_point_add_and_double_64(init1, p0, p1);
          swap1[0U] = bit;
        }
      }
      Hacl_Impl_Curve25519_Generic_cswap_64(swap1[0U], p0, p1);
      Hacl_Impl_Curve25519_Generic_point_double_64(p0);
      Hacl_Impl_Curve25519_Generic_point_double_64(p0);
      Hacl_Impl_Curve25519_Generic_point_double_64(p0);
      memcpy(out, p0, (uint32_t)2U * (uint32_t)4U * sizeof p0[0U]);
    }
  }
}

static uint8_t
Hacl_Impl_Curve25519_Generic_g25519[32U] =
  {
    (uint8_t)9U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U
  };

void Hacl_Curve25519_64_secret_to_public(uint8_t *pub, uint8_t *priv)
{
  uint8_t basepoint[32U] = { 0U };
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)32U; i = i + (uint32_t)1U)
    {
      basepoint[i] = Hacl_Impl_Curve25519_Generic_g25519[i];
    }
  }
  {
    uint64_t scalar[4U] = { 0U };
    {
      uint8_t *b_i = priv + (uint32_t)0U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[0U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)1U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[1U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)2U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[2U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)3U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[3U] = u_i;
    }
    scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
    scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
    scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
    {
      uint64_t init1[(uint32_t)2U * (uint32_t)4U];
      memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      Hacl_Impl_Curve25519_Generic_decode_point_64(init1, basepoint);
      Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
      Hacl_Impl_Curve25519_Generic_encode_point_64(pub, init1);
    }
  }
}

bool curve25519_vale64(uint8_t *shared, uint8_t *my_priv, uint8_t *their_pub)
{
  uint64_t scalar[4U] = { 0U };
  {
    uint8_t *b_i = my_priv + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[0U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[1U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[2U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[3U] = u_i;
  }
  scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
  scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
  scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
  {
    uint64_t init1[(uint32_t)2U * (uint32_t)4U];
    memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
    Hacl_Impl_Curve25519_Generic_decode_point_64(init1, their_pub);
    Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
    Hacl_Impl_Curve25519_Generic_encode_point_64(shared, init1);
  }
}

